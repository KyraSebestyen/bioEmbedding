{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "finetune.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOAovvnKUXcTOgAjJvyJ72d",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KyraSebestyen/bioEmbedding/blob/main/finetune.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDcDbcxiXpjU",
        "outputId": "069c2605-5b0e-45f3-c7e3-dd444901e151"
      },
      "source": [
        "# connect to google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oBPnJQf_btHd",
        "outputId": "742b4fc5-4d9c-418d-cf86-939c38923029"
      },
      "source": [
        "!pip install datasets"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-1.17.0-py3-none-any.whl (306 kB)\n",
            "\u001b[K     |████████████████████████████████| 306 kB 5.3 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.2.1-py3-none-any.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 398 kB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.62.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.8.2)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n",
            "Collecting fsspec[http]>=2021.05.0\n",
            "  Downloading fsspec-2021.11.1-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 53.2 MB/s \n",
            "\u001b[?25hCollecting aiohttp\n",
            "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 45.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyarrow!=4.0.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n",
            "\u001b[K     |████████████████████████████████| 243 kB 51.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.4.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.13)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.6)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 50.6 MB/s \n",
            "\u001b[?25hCollecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (192 kB)\n",
            "\u001b[K     |████████████████████████████████| 192 kB 54.6 MB/s \n",
            "\u001b[?25hCollecting asynctest==0.13.0\n",
            "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.0.9)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-5.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (160 kB)\n",
            "\u001b[K     |████████████████████████████████| 160 kB 51.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.2.0)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.6.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: multidict, frozenlist, yarl, asynctest, async-timeout, aiosignal, fsspec, aiohttp, xxhash, huggingface-hub, datasets\n",
            "Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 datasets-1.17.0 frozenlist-1.2.0 fsspec-2021.11.1 huggingface-hub-0.2.1 multidict-5.2.0 xxhash-2.0.2 yarl-1.7.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ml4E_AZjW0cN",
        "outputId": "d5976d7d-0c68-4ac8-f89b-54d17129b8b4"
      },
      "source": [
        "!pip install transformers\n",
        "import pandas\n",
        "import numpy\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.15.0-py3-none-any.whl (3.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.4 MB 5.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 44.7 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 46.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.2.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 50.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed pyyaml-6.0 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJ7QFW5VXT7J",
        "outputId": "6a7003dd-5c2f-42be-eb38-624b22b85fae"
      },
      "source": [
        "!git clone https://github.com/huggingface/transformers\n",
        "!pip install ./transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'transformers'...\n",
            "remote: Enumerating objects: 94910, done.\u001b[K\n",
            "remote: Total 94910 (delta 0), reused 0 (delta 0), pack-reused 94910\u001b[K\n",
            "Receiving objects: 100% (94910/94910), 80.17 MiB | 24.37 MiB/s, done.\n",
            "Resolving deltas: 100% (68905/68905), done.\n",
            "Processing ./transformers\n",
            "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
            "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.0.dev0) (6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.0.dev0) (1.19.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.0.dev0) (21.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.0.dev0) (4.8.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.0.dev0) (2019.12.20)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.0.dev0) (0.2.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.0.dev0) (0.0.46)\n",
            "Requirement already satisfied: tokenizers>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.0.dev0) (0.10.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.0.dev0) (3.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.0.dev0) (4.62.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.0.dev0) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.16.0.dev0) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers==4.16.0.dev0) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.16.0.dev0) (3.6.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.16.0.dev0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.16.0.dev0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.16.0.dev0) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.16.0.dev0) (1.24.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.16.0.dev0) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.16.0.dev0) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.16.0.dev0) (7.1.2)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.16.0.dev0-py3-none-any.whl size=3387433 sha256=eedcd74faa64eca792b60aa335a5003735468fef1b81237b94e8071ba3bbeb3b\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-ponq4gex/wheels/49/62/f4/6730819eed4e6468662b1519bf3bf46419b2335990c77f8767\n",
            "Successfully built transformers\n",
            "Installing collected packages: transformers\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.15.0\n",
            "    Uninstalling transformers-4.15.0:\n",
            "      Successfully uninstalled transformers-4.15.0\n",
            "Successfully installed transformers-4.16.0.dev0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wy3_BK5zX1Cy"
      },
      "source": [
        "corpus = pandas.read_csv('/content/gdrive/My Drive/Colab_Notebooks/Okkurrenzen Auszug.csv', sep = ';', usecols = [\"text\"], quoting = 3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVLc1e7lYSXs"
      },
      "source": [
        "trainCorpus, validCorpus = train_test_split(corpus.text.values, test_size = 0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jouoOJqGZfLi"
      },
      "source": [
        "with open(\"train.txt\", \"w\") as file:\n",
        "  for document in trainCorpus:\n",
        "    file.write(document + \"\\n\")\n",
        "with open(\"valid.txt\", \"w\") as file:\n",
        "  for document in validCorpus:\n",
        "    file.write(document + \"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-j6PtLHZ9uM",
        "outputId": "0d9beede-4826-4ec0-dec6-1790707e6ec8"
      },
      "source": [
        "mlmScriptPath = \"transformers/examples/pytorch/language-modeling/run_mlm.py\"\n",
        "modelName = \"bert-base-uncased\"\n",
        "trainFilePath = \"train.txt\"\n",
        "validFilePath = \"valid.txt\"\n",
        "outputDir = \"finetuned_model\"\n",
        "epochs = 100\n",
        "saveSteps = 1000\n",
        "evalSteps = 500\n",
        "\n",
        "!python3 {mlmScriptPath} \\\n",
        "        --train_file {trainFilePath} \\\n",
        "        --validation_file {validFilePath} \\\n",
        "        --model_name_or_path {modelName} \\\n",
        "        --output_dir {outputDir} \\\n",
        "        --do_train \\\n",
        "        --do_eval \\\n",
        "        --evaluation_strategy steps \\\n",
        "        --disable_tqdm True \\\n",
        "        --save_steps {saveSteps} \\\n",
        "        --eval_steps {evalSteps} \\\n",
        "        --num_train_epochs {epochs} "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "01/06/2022 08:45:48 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "01/06/2022 08:45:48 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=True,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_steps=500,\n",
            "evaluation_strategy=IntervalStrategy.STEPS,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=finetuned_model/runs/Jan06_08-45-48_4251fee62205,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=100.0,\n",
            "output_dir=finetuned_model,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=8,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=finetuned_model,\n",
            "save_on_each_node=False,\n",
            "save_steps=1000,\n",
            "save_strategy=IntervalStrategy.STEPS,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "01/06/2022 08:45:48 - INFO - __main__ - Checkpoint detected, resuming training at finetuned_model/checkpoint-13000. To avoid this behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\n",
            "01/06/2022 08:45:48 - WARNING - datasets.builder - Using custom data configuration default-d2fa41bbdee9ae86\n",
            "01/06/2022 08:45:48 - INFO - datasets.builder - Generating dataset text (/root/.cache/huggingface/datasets/text/default-d2fa41bbdee9ae86/0.0.0/d86c40dad297bdddf277b406c6a59f0250b5318c400bf23d420a31aff88c84c4)\n",
            "Downloading and preparing dataset text/default to /root/.cache/huggingface/datasets/text/default-d2fa41bbdee9ae86/0.0.0/d86c40dad297bdddf277b406c6a59f0250b5318c400bf23d420a31aff88c84c4...\n",
            "\r  0% 0/2 [00:00<?, ?it/s]\r100% 2/2 [00:00<00:00, 7108.99it/s]\n",
            "01/06/2022 08:45:48 - INFO - datasets.utils.download_manager - Downloading took 0.0 min\n",
            "01/06/2022 08:45:48 - INFO - datasets.utils.download_manager - Checksum Computation took 0.0 min\n",
            "\r  0% 0/2 [00:00<?, ?it/s]\r100% 2/2 [00:00<00:00, 1083.94it/s]\n",
            "01/06/2022 08:45:48 - INFO - datasets.utils.info_utils - Unable to verify checksums.\n",
            "01/06/2022 08:45:48 - INFO - datasets.builder - Generating split train\n",
            "01/06/2022 08:45:48 - INFO - datasets.builder - Generating split validation\n",
            "01/06/2022 08:45:48 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
            "Dataset text downloaded and prepared to /root/.cache/huggingface/datasets/text/default-d2fa41bbdee9ae86/0.0.0/d86c40dad297bdddf277b406c6a59f0250b5318c400bf23d420a31aff88c84c4. Subsequent calls will reuse this data.\n",
            "\r  0% 0/2 [00:00<?, ?it/s]\r100% 2/2 [00:00<00:00, 823.87it/s]\n",
            "[INFO|file_utils.py:2042] 2022-01-06 08:45:49,017 >> https://huggingface.co/bert-base-uncased/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpjmq4e3uw\n",
            "Downloading: 100% 570/570 [00:00<00:00, 485kB/s]\n",
            "[INFO|file_utils.py:2046] 2022-01-06 08:45:49,158 >> storing https://huggingface.co/bert-base-uncased/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "[INFO|file_utils.py:2054] 2022-01-06 08:45:49,158 >> creating metadata file for /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "[INFO|configuration_utils.py:609] 2022-01-06 08:45:49,159 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "[INFO|configuration_utils.py:645] 2022-01-06 08:45:49,159 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.16.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|file_utils.py:2042] 2022-01-06 08:45:49,442 >> https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpyenqzsey\n",
            "Downloading: 100% 28.0/28.0 [00:00<00:00, 24.2kB/s]\n",
            "[INFO|file_utils.py:2046] 2022-01-06 08:45:49,583 >> storing https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
            "[INFO|file_utils.py:2054] 2022-01-06 08:45:49,584 >> creating metadata file for /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
            "[INFO|configuration_utils.py:609] 2022-01-06 08:45:49,856 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "[INFO|configuration_utils.py:645] 2022-01-06 08:45:49,857 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.16.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|file_utils.py:2042] 2022-01-06 08:45:50,129 >> https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp6lksamzb\n",
            "Downloading: 100% 226k/226k [00:00<00:00, 1.99MB/s]\n",
            "[INFO|file_utils.py:2046] 2022-01-06 08:45:50,383 >> storing https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "[INFO|file_utils.py:2054] 2022-01-06 08:45:50,383 >> creating metadata file for /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "[INFO|file_utils.py:2042] 2022-01-06 08:45:50,524 >> https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp5edvjby1\n",
            "Downloading: 100% 455k/455k [00:00<00:00, 3.35MB/s]\n",
            "[INFO|file_utils.py:2046] 2022-01-06 08:45:50,800 >> storing https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
            "[INFO|file_utils.py:2054] 2022-01-06 08:45:50,801 >> creating metadata file for /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
            "[INFO|tokenization_utils_base.py:1740] 2022-01-06 08:45:51,201 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "[INFO|tokenization_utils_base.py:1740] 2022-01-06 08:45:51,201 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
            "[INFO|tokenization_utils_base.py:1740] 2022-01-06 08:45:51,201 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1740] 2022-01-06 08:45:51,201 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1740] 2022-01-06 08:45:51,201 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
            "[INFO|configuration_utils.py:609] 2022-01-06 08:45:51,472 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "[INFO|configuration_utils.py:645] 2022-01-06 08:45:51,473 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.16.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|file_utils.py:2042] 2022-01-06 08:45:51,649 >> https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp1tcy_s3j\n",
            "Downloading: 100% 420M/420M [00:10<00:00, 43.7MB/s]\n",
            "[INFO|file_utils.py:2046] 2022-01-06 08:46:01,831 >> storing https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
            "[INFO|file_utils.py:2054] 2022-01-06 08:46:01,831 >> creating metadata file for /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
            "[INFO|modeling_utils.py:1353] 2022-01-06 08:46:01,831 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
            "[WARNING|modeling_utils.py:1612] 2022-01-06 08:46:04,011 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[INFO|modeling_utils.py:1629] 2022-01-06 08:46:04,011 >> All the weights of BertForMaskedLM were initialized from the model checkpoint at bert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.\n",
            "Running tokenizer on every text in dataset:   0% 0/19 [00:00<?, ?ba/s][WARNING|tokenization_utils_base.py:3347] 2022-01-06 08:46:04,513 >> Token indices sequence length is longer than the specified maximum sequence length for this model (1014 > 512). Running this sequence through the model will result in indexing errors\n",
            "01/06/2022 08:46:04 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/text/default-d2fa41bbdee9ae86/0.0.0/d86c40dad297bdddf277b406c6a59f0250b5318c400bf23d420a31aff88c84c4/cache-996ec15383531def.arrow\n",
            "Running tokenizer on every text in dataset: 100% 19/19 [00:02<00:00,  8.54ba/s]\n",
            "Running tokenizer on every text in dataset:   0% 0/3 [00:00<?, ?ba/s]01/06/2022 08:46:06 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/text/default-d2fa41bbdee9ae86/0.0.0/d86c40dad297bdddf277b406c6a59f0250b5318c400bf23d420a31aff88c84c4/cache-01ad54eff093ba65.arrow\n",
            "Running tokenizer on every text in dataset: 100% 3/3 [00:00<00:00, 14.07ba/s]\n",
            "Grouping texts in chunks of 512:   0% 0/19 [00:00<?, ?ba/s]01/06/2022 08:46:06 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/text/default-d2fa41bbdee9ae86/0.0.0/d86c40dad297bdddf277b406c6a59f0250b5318c400bf23d420a31aff88c84c4/cache-590175cc8f59c861.arrow\n",
            "Grouping texts in chunks of 512: 100% 19/19 [00:01<00:00, 14.28ba/s]\n",
            "Grouping texts in chunks of 512:   0% 0/3 [00:00<?, ?ba/s]01/06/2022 08:46:08 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/text/default-d2fa41bbdee9ae86/0.0.0/d86c40dad297bdddf277b406c6a59f0250b5318c400bf23d420a31aff88c84c4/cache-df744d73f3fc58c5.arrow\n",
            "Grouping texts in chunks of 512: 100% 3/3 [00:00<00:00, 18.44ba/s]\n",
            "[INFO|trainer.py:1093] 2022-01-06 08:46:19,336 >> Loading model from finetuned_model/checkpoint-13000).\n",
            "[INFO|trainer.py:550] 2022-01-06 08:46:20,483 >> The following columns in the training set  don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[INFO|trainer.py:1208] 2022-01-06 08:46:21,030 >> ***** Running training *****\n",
            "[INFO|trainer.py:1209] 2022-01-06 08:46:21,030 >>   Num examples = 1334\n",
            "[INFO|trainer.py:1210] 2022-01-06 08:46:21,030 >>   Num Epochs = 100\n",
            "[INFO|trainer.py:1211] 2022-01-06 08:46:21,030 >>   Instantaneous batch size per device = 8\n",
            "[INFO|trainer.py:1212] 2022-01-06 08:46:21,030 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "[INFO|trainer.py:1213] 2022-01-06 08:46:21,031 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1214] 2022-01-06 08:46:21,031 >>   Total optimization steps = 16700\n",
            "[INFO|trainer.py:1234] 2022-01-06 08:46:21,031 >>   Continuing training from checkpoint, will skip to saved global_step\n",
            "[INFO|trainer.py:1235] 2022-01-06 08:46:21,031 >>   Continuing training from epoch 77\n",
            "[INFO|trainer.py:1236] 2022-01-06 08:46:21,031 >>   Continuing training from global step 13000\n",
            "[INFO|trainer.py:1239] 2022-01-06 08:46:21,031 >>   Will skip the first 77 epochs then the first 141 batches in the first epoch. If this takes a lot of time, you can add the `--ignore_data_skip` flag to your launch command, but you will resume the training on data already seen by your model.\n",
            "{'loss': 1.1041, 'learning_rate': 9.580838323353295e-06, 'epoch': 80.84}\n",
            "[INFO|trainer.py:550] 2022-01-06 09:05:26,130 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[INFO|trainer.py:2289] 2022-01-06 09:05:26,133 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2291] 2022-01-06 09:05:26,133 >>   Num examples = 145\n",
            "[INFO|trainer.py:2294] 2022-01-06 09:05:26,133 >>   Batch size = 8\n",
            "{'eval_loss': 0.9258964657783508, 'eval_runtime': 14.8571, 'eval_samples_per_second': 9.76, 'eval_steps_per_second': 1.279, 'epoch': 80.84}\n",
            "{'loss': 1.0698, 'learning_rate': 8.083832335329342e-06, 'epoch': 83.83}\n",
            "[INFO|trainer.py:550] 2022-01-06 09:24:42,887 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[INFO|trainer.py:2289] 2022-01-06 09:24:42,890 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2291] 2022-01-06 09:24:42,890 >>   Num examples = 145\n",
            "[INFO|trainer.py:2294] 2022-01-06 09:24:42,890 >>   Batch size = 8\n",
            "{'eval_loss': 0.9475288987159729, 'eval_runtime': 14.8604, 'eval_samples_per_second': 9.757, 'eval_steps_per_second': 1.279, 'epoch': 83.83}\n",
            "[INFO|trainer.py:2039] 2022-01-06 09:24:57,751 >> Saving model checkpoint to finetuned_model/checkpoint-14000\n",
            "[INFO|configuration_utils.py:426] 2022-01-06 09:24:57,752 >> Configuration saved in finetuned_model/checkpoint-14000/config.json\n",
            "[INFO|modeling_utils.py:1067] 2022-01-06 09:24:58,872 >> Model weights saved in finetuned_model/checkpoint-14000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2043] 2022-01-06 09:24:58,872 >> tokenizer config file saved in finetuned_model/checkpoint-14000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2049] 2022-01-06 09:24:58,872 >> Special tokens file saved in finetuned_model/checkpoint-14000/special_tokens_map.json\n",
            "{'loss': 1.0491, 'learning_rate': 6.58682634730539e-06, 'epoch': 86.83}\n",
            "[INFO|trainer.py:550] 2022-01-06 09:44:05,631 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[INFO|trainer.py:2289] 2022-01-06 09:44:05,634 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2291] 2022-01-06 09:44:05,634 >>   Num examples = 145\n",
            "[INFO|trainer.py:2294] 2022-01-06 09:44:05,634 >>   Batch size = 8\n",
            "{'eval_loss': 0.9379798769950867, 'eval_runtime': 14.8735, 'eval_samples_per_second': 9.749, 'eval_steps_per_second': 1.277, 'epoch': 86.83}\n",
            "{'loss': 1.0383, 'learning_rate': 5.0898203592814375e-06, 'epoch': 89.82}\n",
            "[INFO|trainer.py:550] 2022-01-06 10:03:24,265 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[INFO|trainer.py:2289] 2022-01-06 10:03:24,267 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2291] 2022-01-06 10:03:24,267 >>   Num examples = 145\n",
            "[INFO|trainer.py:2294] 2022-01-06 10:03:24,267 >>   Batch size = 8\n",
            "{'eval_loss': 0.939871609210968, 'eval_runtime': 14.8634, 'eval_samples_per_second': 9.756, 'eval_steps_per_second': 1.278, 'epoch': 89.82}\n",
            "[INFO|trainer.py:2039] 2022-01-06 10:03:39,131 >> Saving model checkpoint to finetuned_model/checkpoint-15000\n",
            "[INFO|configuration_utils.py:426] 2022-01-06 10:03:39,132 >> Configuration saved in finetuned_model/checkpoint-15000/config.json\n",
            "[INFO|modeling_utils.py:1067] 2022-01-06 10:03:40,627 >> Model weights saved in finetuned_model/checkpoint-15000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2043] 2022-01-06 10:03:40,628 >> tokenizer config file saved in finetuned_model/checkpoint-15000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2049] 2022-01-06 10:03:40,628 >> Special tokens file saved in finetuned_model/checkpoint-15000/special_tokens_map.json\n",
            "{'loss': 1.0325, 'learning_rate': 3.592814371257485e-06, 'epoch': 92.81}\n",
            "[INFO|trainer.py:550] 2022-01-06 10:22:59,970 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[INFO|trainer.py:2289] 2022-01-06 10:22:59,973 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2291] 2022-01-06 10:22:59,973 >>   Num examples = 145\n",
            "[INFO|trainer.py:2294] 2022-01-06 10:22:59,973 >>   Batch size = 8\n",
            "{'eval_loss': 0.9395803809165955, 'eval_runtime': 15.0416, 'eval_samples_per_second': 9.64, 'eval_steps_per_second': 1.263, 'epoch': 92.81}\n",
            "{'loss': 1.0261, 'learning_rate': 2.095808383233533e-06, 'epoch': 95.81}\n",
            "[INFO|trainer.py:550] 2022-01-06 10:42:30,440 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[INFO|trainer.py:2289] 2022-01-06 10:42:30,443 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2291] 2022-01-06 10:42:30,443 >>   Num examples = 145\n",
            "[INFO|trainer.py:2294] 2022-01-06 10:42:30,443 >>   Batch size = 8\n",
            "{'eval_loss': 0.9490286111831665, 'eval_runtime': 15.0389, 'eval_samples_per_second': 9.642, 'eval_steps_per_second': 1.263, 'epoch': 95.81}\n",
            "[INFO|trainer.py:2039] 2022-01-06 10:42:45,483 >> Saving model checkpoint to finetuned_model/checkpoint-16000\n",
            "[INFO|configuration_utils.py:426] 2022-01-06 10:42:45,484 >> Configuration saved in finetuned_model/checkpoint-16000/config.json\n",
            "[INFO|modeling_utils.py:1067] 2022-01-06 10:42:46,710 >> Model weights saved in finetuned_model/checkpoint-16000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2043] 2022-01-06 10:42:46,711 >> tokenizer config file saved in finetuned_model/checkpoint-16000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2049] 2022-01-06 10:42:46,711 >> Special tokens file saved in finetuned_model/checkpoint-16000/special_tokens_map.json\n",
            "{'loss': 1.0061, 'learning_rate': 5.988023952095809e-07, 'epoch': 98.8}\n",
            "[INFO|trainer.py:550] 2022-01-06 11:02:05,488 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[INFO|trainer.py:2289] 2022-01-06 11:02:05,490 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2291] 2022-01-06 11:02:05,490 >>   Num examples = 145\n",
            "[INFO|trainer.py:2294] 2022-01-06 11:02:05,490 >>   Batch size = 8\n",
            "{'eval_loss': 0.9174379706382751, 'eval_runtime': 15.0344, 'eval_samples_per_second': 9.645, 'eval_steps_per_second': 1.264, 'epoch': 98.8}\n",
            "[INFO|trainer.py:1429] 2022-01-06 11:10:02,347 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 8621.3274, 'train_samples_per_second': 15.473, 'train_steps_per_second': 1.937, 'train_loss': 0.23148211542003883, 'epoch': 100.0}\n",
            "[INFO|trainer.py:2039] 2022-01-06 11:10:02,360 >> Saving model checkpoint to finetuned_model\n",
            "[INFO|configuration_utils.py:426] 2022-01-06 11:10:02,361 >> Configuration saved in finetuned_model/config.json\n",
            "[INFO|modeling_utils.py:1067] 2022-01-06 11:10:03,670 >> Model weights saved in finetuned_model/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2043] 2022-01-06 11:10:03,671 >> tokenizer config file saved in finetuned_model/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2049] 2022-01-06 11:10:03,671 >> Special tokens file saved in finetuned_model/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =      100.0\n",
            "  train_loss               =     0.2315\n",
            "  train_runtime            = 2:23:41.32\n",
            "  train_samples            =       1334\n",
            "  train_samples_per_second =     15.473\n",
            "  train_steps_per_second   =      1.937\n",
            "01/06/2022 11:10:03 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:550] 2022-01-06 11:10:03,787 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[INFO|trainer.py:2289] 2022-01-06 11:10:03,789 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2291] 2022-01-06 11:10:03,789 >>   Num examples = 145\n",
            "[INFO|trainer.py:2294] 2022-01-06 11:10:03,789 >>   Batch size = 8\n",
            "{'eval_loss': 0.97223961353302, 'eval_runtime': 15.0363, 'eval_samples_per_second': 9.643, 'eval_steps_per_second': 1.264, 'epoch': 100.0}\n",
            "***** eval metrics *****\n",
            "  epoch                   =      100.0\n",
            "  eval_loss               =     0.9722\n",
            "  eval_runtime            = 0:00:15.03\n",
            "  eval_samples            =        145\n",
            "  eval_samples_per_second =      9.643\n",
            "  eval_steps_per_second   =      1.264\n",
            "  perplexity              =     2.6439\n",
            "[INFO|modelcard.py:460] 2022-01-06 11:10:18,995 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Masked Language Modeling', 'type': 'fill-mask'}}\n"
          ]
        }
      ]
    }
  ]
}